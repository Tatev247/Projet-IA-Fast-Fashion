{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X12X-5r4tYJM"
   },
   "source": [
    "## **Importer les librairies nécessaires**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "zekaZdY3tYJM"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import requests\n",
    "\n",
    "import pandas as pd\n",
    "import numpy  as np\n",
    "\n",
    "from datetime   import datetime\n",
    "from bs4        import BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gT-65OkgtYJN"
   },
   "source": [
    "## **Extraction**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AOyiePY5tYJO"
   },
   "source": [
    "#### 1. **Extraction des url pour chacun des articles présents sur la page**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "SSD6w6P9tYJO"
   },
   "outputs": [],
   "source": [
    "# Requesting\n",
    "url = 'https://www2.hm.com/en_us/men/products/jeans.html' # h&m url\n",
    "headers = {'User-Agent' : 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/108.0.0.0 Safari/537.36'} # user agent\n",
    "page = requests.get(url, headers=headers) \n",
    "\n",
    "# BeautifulSoup Object\n",
    "soup = BeautifulSoup(page.text, 'html.parser')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q7fnTwaatYJP"
   },
   "source": [
    "Du fait, que tous les articles ne s'affiche pas sur une seule et même page, nous devons trouver la taille de page maximale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "knwDeBSNtYJP"
   },
   "outputs": [],
   "source": [
    "# Paging\n",
    "total_item = soup.find_all('h2', class_='load-more-heading')[0].get('data-total') # nombre d'items disponibles\n",
    "\n",
    "page_number = np.round(int(total_item)/ 36) + 1 # nombre de page nécessaire (arrondi) \n",
    "url_complete = url + '?page-size=' + str(int(page_number*36)) # nouvelle url avec tous les items sur la même page\n",
    "\n",
    "# Nouvelle requête\n",
    "url = url_complete # url complete du site H&M\n",
    "headers = {'User-Agent' : 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/108.0.0.0 Safari/537.36'}\n",
    "page = requests.get(url, headers=headers)\n",
    "\n",
    "soup = BeautifulSoup(page.text, 'html.parser') \n",
    "products = soup.find('ul', class_='products-listing small') # trouver la liste complète des produits présents sur le site via la balise ul ayant pour classe 'products-listing small'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P5mP4jxMtYJQ"
   },
   "source": [
    "Cela nous permets de collecter les informations concernant l'identifiant du produits pour la suite, ainsi que du type de produit, puisque ces informations ne sont pas disponibles sur les liens de la fiche produits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "0r8892OwtYJQ"
   },
   "outputs": [],
   "source": [
    "# Récupération des informations\n",
    "product_list = products.find_all('article', class_='hm-product-item') # on obtient chaque produit via la balise article et la classe 'hm-product-item'\n",
    "\n",
    "# product id\n",
    "product_id = [p.get('data-articlecode') for p in product_list]\n",
    "\n",
    "# product_type\n",
    "product_type = [p.get('data-category') for p in product_list]\n",
    "\n",
    "# Construction du tableau initial\n",
    "data = pd.DataFrame([product_id, product_type]).T\n",
    "data.columns = ['product_id', 'product_type']\n",
    "\n",
    "# Ajustement initiaux\n",
    "data['product_id'] = data['product_id'].astype(str)\n",
    "data['style_id'] = data['product_id'].apply(lambda x: x[:-3])\n",
    "data['color_id'] = data['product_id'].apply(lambda x: x[-3:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4O32KR4ItYJR"
   },
   "source": [
    "#### 2. **Extraction d'autres attributs pour chaque produit présent sur le site internet**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9uEYBo0ttYJR"
   },
   "outputs": [],
   "source": [
    "headers = {'User-Agent' : 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/108.0.0.0 Safari/537.36'} # user agent\n",
    "\n",
    "# Tableau de données de référence vide\n",
    "df_compositions = pd.DataFrame()\n",
    "\n",
    "# Requête API\n",
    "for i in range(len(data)):\n",
    "    url = 'https://www2.hm.com/en_us/productpage.' + data.loc[i, 'product_id'] + '.html'\n",
    "\n",
    "    page = requests.get(url, headers=headers)\n",
    "\n",
    "    soup = BeautifulSoup(page.text, 'html.parser') \n",
    "\n",
    "    ############################################################################################ Extraction des couleurs ########################################################################################\n",
    "    id_color_list = soup.find_all('a', class_='filter-option miniature active') + soup.find_all('a', class_='filter-option miniature')\n",
    "\n",
    "    color_name = [p.get('data-color') for p in id_color_list] # obtention des couleurs\n",
    "    product_id = [p.get('data-articlecode') for p in id_color_list] # obtention des ID - Chaque couleur à un identifiant unique ID (3 derniers chiffres)\n",
    "\n",
    "    df_color = pd.DataFrame([product_id, color_name]).T\n",
    "    df_color.columns = ['product_id', 'color_name']\n",
    "\n",
    "    for j in range(len(df_color)):\n",
    "        ####################################### Requête API #######################################\n",
    "        url = 'https://www2.hm.com/en_us/productpage.' + df_color.loc[j, 'product_id'] + '.html'\n",
    "        page = requests.get(url, headers=headers)    \n",
    "        soup = BeautifulSoup(page.text, 'html.parser') \n",
    "        \n",
    "        ####################################### Nom du produit #######################################\n",
    "        product_name = soup.find_all('hm-product-name', id='js-product-name')\n",
    "        product_name = product_name[0].get_text().strip()\n",
    "        \n",
    "        product_price = soup.find_all('div', class_='price parbase')\n",
    "        product_price = product_price[0].get_text().strip()\n",
    "\n",
    "        ######################################################################################## Extraction de la composition des articles #############################################################################\n",
    "        product_composition_list = soup.find_all('div', class_='details-attributes-list-item') # attributes list\n",
    "        product_composition = [list(filter(None, item.get_text().split('\\n'))) for item in product_composition_list]\n",
    "        \n",
    "        if product_composition != []: # si l'information relation a la composition n'est pas vide\n",
    "            df_composition_ref = pd.DataFrame(product_composition).T # On crée un tableau à partir de la liste product_composition\n",
    "            df_composition_ref.columns = df_composition_ref.iloc[0, :] # Définir la première comme une colonne\n",
    "            df_composition = df_composition_ref[['Fit', 'Composition', 'Art. No.']] # Sélectionner que les colonnes nécessaires\n",
    "            df_composition = df_composition[df_composition['Composition'].notnull()]\n",
    "            df_composition = df_composition.iloc[1:].fillna(method='ffill') # Gestion des valeurs nulles\n",
    "\n",
    "            # \n",
    "            df_composition['Composition'] = df_composition['Composition'].replace('Shell: ', '', regex=True)\n",
    "\n",
    "            # Renommer les  colonnes\n",
    "            df_composition = df_composition.rename(columns = {'Fit' : 'fit', 'Composition' : 'composition', 'Art. No.' : 'product_id'})\n",
    "\n",
    "            # Ajout des colonnes product_name et product_price\n",
    "            df_composition['product_name'] = product_name\n",
    "            df_composition['product_price'] = product_price\n",
    "            ######################################################################################## Fusion #####################################################################################################\n",
    "            # Color + Composition\n",
    "            df_composition = pd.merge(df_composition, df_color, how='left', on='product_id')\n",
    "\n",
    "            # Attributs\n",
    "            df_compositions = pd.concat([df_compositions, df_composition], axis=0)\n",
    "        \n",
    "        else: # Si c'est vide\n",
    "            None\n",
    "\n",
    "# Génére Style ID + Color ID\n",
    "df_compositions['style_id'] = df_compositions['product_id'].apply(lambda x: x[:-3]) # product_id = style_id + color_id\n",
    "df_compositions['color_id'] = df_compositions['product_id'].apply(lambda x: x[-3:])\n",
    "\n",
    "# sDate et heure du scrapping\n",
    "df_compositions['scraping_datetime'] = datetime.now().strftime('%Y-%m-%d %H:%M:%S')        \n",
    "\n",
    "# Fusion\n",
    "df_raw = pd.merge(data[['product_type', 'style_id']], df_compositions, how='left', on='style_id')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kLoCIt80tYJS"
   },
   "source": [
    "#### 3. **Fusion de product_type et df_compositions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fa0jSSdWtYJS"
   },
   "outputs": [],
   "source": [
    "df_raw = pd.merge(data[['product_type', 'style_id']], df_compositions, how='left', on='style_id')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Rp6p68ZqtYJS"
   },
   "source": [
    "## 3. **Nettoyage du jeu de données**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424
    },
    "id": "L-MRJraQtYJS",
    "outputId": "003e476d-618b-45f4-cf92-aa81ab58f200"
   },
   "outputs": [],
   "source": [
    "df_raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wQGa-6W6tYJT",
    "outputId": "2b8e6b0e-6b18-42ec-a766-24495e6c2a03"
   },
   "outputs": [],
   "source": [
    "print(df_raw.dtypes)\n",
    "print()\n",
    "print(df_raw.isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "--71xTbPtYJU"
   },
   "outputs": [],
   "source": [
    "# Renommer les colonnes\n",
    "df_raw.rename(columns = {'fit' : 'product_fit', 'color_name' : 'product_color', 'composition' : 'product_composition'}, inplace=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dMZyzhTOtYJU"
   },
   "outputs": [],
   "source": [
    "df_raw['product_color'] = df_raw['product_color'].apply(lambda x: x.replace(' ', '_').lower() if pd.notnull(x) else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8HOjq1_cuwmT"
   },
   "outputs": [],
   "source": [
    "df_raw['product_fit'] = df_raw['product_fit'].apply(lambda x: x.replace(' ', '_').lower() if pd.notnull(x) else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ytMfx_xetYJU"
   },
   "outputs": [],
   "source": [
    "df_raw['product_name'] = df_raw['product_name'].apply(lambda x: x.replace(' ', '_').lower() if pd.notnull(x) else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 397
    },
    "id": "2qFBQzSFtYJV",
    "outputId": "c2a2a308-cf0a-4d41-e6b0-cee177ba7c74"
   },
   "outputs": [],
   "source": [
    "df_raw['product_price'] = df_raw['product_price'].apply(lambda x: x.replace('$', ' ') if pd.notnull(x) else x)\n",
    "df_raw['product_price'] = pd.to_numeric(df_raw['product_price'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qUXG_S1RtYJV"
   },
   "outputs": [],
   "source": [
    "df_raw['scraping_datetime'] = pd.to_datetime(df_raw['scraping_datetime'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Jqo_iROKtYJV"
   },
   "source": [
    "###**product_composition**\n",
    "\n",
    "###### Séparation des éléments contenus dans la colonnes product_composition (i.e : Polyamide 64%, Spandex 36%)\n",
    "###### d'autre part, on supprime certaines lignes (i.e: Pocket Lining\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pbKOKvjrtYJV"
   },
   "outputs": [],
   "source": [
    "# Suppression de toutes les lignes contenant Pocket lining\n",
    "df_raw = df_raw[~df_raw['product_composition'].str.contains('Pocket lining', na=False)] \n",
    "\n",
    "# Suppression de toutes les lignes contenant Lining\n",
    "df_raw = df_raw[~df_raw['product_composition'].str.contains('Lining', na=False)] \n",
    "\n",
    "# Suppression de toutes les lignes contenant Pocket\n",
    "df_raw = df_raw[~df_raw['product_composition'].str.contains('Pocket', na=False)].reset_index().drop(columns=['index']) \n",
    "\n",
    "\n",
    "df_aux = df_raw['product_composition'].str.split(',', expand=True) # Colonne auxiliaire"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7ro0kUCUtYJW",
    "outputId": "3cce5f45-bc81-4abc-deaa-f518b16b5267"
   },
   "outputs": [],
   "source": [
    "df_aux[0].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LQmjkQeEtYJW",
    "outputId": "7316e2f2-434f-4d41-8408-d2ebe4deca17"
   },
   "outputs": [],
   "source": [
    "df_aux[1].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lOASs0i0tYJW",
    "outputId": "320635d9-223e-4daf-d735-7d258d243928"
   },
   "outputs": [],
   "source": [
    "df_aux[2].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PcaYozUotYJW"
   },
   "source": [
    "####Récupération de chaque composition et on les mets dans les bonnes colonnes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TUq9fV7ltYJf"
   },
   "outputs": [],
   "source": [
    "# Cotton\n",
    "# On met des valeurs de coton sur la colonne 'coton' si la première colonne de df1 contient des 'Coton', sinon on mets des NaN\n",
    "df_aux['cotton'] = np.where(df_aux[0].str.contains('Cotton'), df_aux[0], np.nan) \n",
    "df_aux['cotton'] = np.where(df_aux[1].str.contains('Cotton'), df_aux[1], df_aux['cotton']) # il y a des valeurs de cotons sur la deuxième colonne\n",
    "\n",
    "#  Spandex\n",
    "# On met des valeurs de spandex sur la colonne 'spandex'si la deuxième colonne de df1 contient des 'Spandex', sinon on mets des NaN\n",
    "df_aux['spandex'] = np.where(df_aux[1].str.contains('Spandex'), df_aux[1], np.nan) # il y a des valeurs de 'spandex'' sur la deuxième colonne\n",
    "df_aux['spandex'] = np.where(df_aux[2].str.contains('Spandex'), df_aux[2], df_aux['spandex']) # il y a des valeurs de 'spandex'' sur la troisième colonne\n",
    "\n",
    "# Polyester\n",
    "df_aux['polyester'] = np.where(df_aux[1].str.contains('Polyester'), df_aux[1], np.nan) # il y a des valeurs de 'polyester'' sur la deuxième colonne\n",
    "\n",
    "# Elastomultiester \n",
    "# On met des valeurs de polyester sur la colonne 'polyester'si la deuxième colonne de df1 contient des 'elastomultiester', sinon on mets des NaN\n",
    "df_aux['elastomultiester'] = np.where(df_aux[1].str.contains('Elastomultiester'), df_aux[1], np.nan) \n",
    "\n",
    "# Lyocell\n",
    "df_aux['lyocell'] = np.where(df_aux[0].str.contains('Lyocell'), df_aux[0], np.nan) \n",
    "df_aux['lyocell'] = np.where(df_aux[1].str.contains('Lyocell'), df_aux[1], df_aux['lyocell']) \n",
    "\n",
    "# Rayon\n",
    "# On met des valeurs de rayon sur la colonne 'rayon'si la deuxième colonne de df1 contient des 'Rayon', sinon on mets des NaN\n",
    "df_aux['rayon'] = np.where(df_aux[0].str.contains('Rayon'), df_aux[0], np.nan) \n",
    "df_aux['rayon'] = np.where(df_aux[2].str.contains('Rayon'), df_aux[2], df_aux['rayon']) # il y a des valeurs de 'rayon'' sur la troisième colonne"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WG0jG4sCtYJg"
   },
   "outputs": [],
   "source": [
    "# Nettoyage\n",
    "\n",
    "# On supprime les colonnes temporaires\n",
    "df_aux = df_aux.drop(columns=[0, 1, 2]) \n",
    "\n",
    "# On concatene df_aux et df_raw\n",
    "df_raw = pd.concat([df_raw, df_aux], axis=1) \n",
    "\n",
    "# On supprime la colonne product_composition\n",
    "df_raw = df_raw.drop(columns=['product_composition']) \n",
    "\n",
    "# Extraction uniquement des nombres présents sur chaque fiche produits\n",
    "df_raw['cotton'] = df_raw['cotton'].apply(lambda x: int(re.search('\\d+', x).group(0)) / 100 if pd.notnull(x) else 0) \n",
    "df_raw['spandex'] = df_raw['spandex'].apply(lambda x: int(re.search('\\d+', x).group(0)) / 100 if pd.notnull(x) else 0)\n",
    "df_raw['polyester'] = df_raw['polyester'].apply(lambda x: int(re.search('\\d+', x).group(0)) / 100 if pd.notnull(x) else 0) \n",
    "df_raw['elastomultiester'] = df_raw['elastomultiester'].apply(lambda x: int(re.search('\\d+', x).group(0)) / 100 if pd.notnull(x) else 0) \n",
    "df_raw['lyocell'] = df_raw['lyocell'].apply(lambda x: int(re.search('\\d+', x).group(0)) / 100 if pd.notnull(x) else 0) \n",
    "df_raw['rayon'] = df_raw['rayon'].apply(lambda x: int(re.search('\\d+', x).group(0)) / 100 if pd.notnull(x) else 0) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0ko-cX-BtYJg"
   },
   "outputs": [],
   "source": [
    "df = df_raw.drop_duplicates().copy() # suppression des doublons\n",
    "\n",
    "#On réarrange les colonnes pour que cela soit plus lisibles\n",
    "df = df[['product_id', 'style_id', 'color_id', 'product_name', 'product_type', \n",
    "         'product_color',  'cotton', 'spandex', 'polyester',\n",
    "         'elastomultiester', 'lyocell', 'rayon', 'product_price', 'scraping_datetime']] # rearranging columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 332
    },
    "id": "SF3o4wHxK9jg",
    "outputId": "2c28bb4f-2906-4d68-dc33-72abd3026b38"
   },
   "outputs": [],
   "source": [
    "# Le tableau final avec toutes les modifications apportées \n",
    "# On affiche que les 10 premiers élements du tableau\n",
    "df.head(9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PFMl9o30tYJh",
    "outputId": "6d36e172-f2fa-4ebf-89d2-c941f2131245"
   },
   "outputs": [],
   "source": [
    "print(df.dtypes)\n",
    "print()\n",
    "print(df.isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17
    },
    "id": "ApWPGIfubm--",
    "outputId": "e8606708-e1fd-4da3-910c-303feb80bb87"
   },
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "df.to_csv('h&m_jeans_homme.csv', encoding='utf-8')\n",
    "files.download('h&m_jeans_homme.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hzg-7FQNb1vw"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "ed3943dba37f3ed717092a780584c496f36863d6c99891baccd6632ecc02cdda"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
